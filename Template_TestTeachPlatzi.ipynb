{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "cm9GN4WNtK_d",
        "kQb9SHI-tNyH",
        "-AKKIC9AzDWP",
        "Ai85jC1HzGSI"
      ],
      "authorship_tag": "ABX9TyMcyjLZFiCwniTWTskuDAOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/test-teach-platzi/blob/main/Template_TestTeachPlatzi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning de Transformers con HuggingFace\n",
        "\n",
        "Author: Axel Sirota"
      ],
      "metadata": {
        "id": "gzf37X_XizQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace es una empresa con una fuerte filosofía de código abierto que hace que los Transformers estén fácilmente disponibles para que no tenga que hacer lo que hicimos antes para cada aplicación."
      ],
      "metadata": {
        "id": "5MjeyOLmr-WF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep"
      ],
      "metadata": {
        "id": "cm9GN4WNtK_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets evaluate transformers transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "C9BTEOu0PerV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fpgYwAtNO2T"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "import keras.backend as K\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "\n",
        "TRACE = False\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizar y cargar el Dataset\n",
        "\n",
        "En HuggingFace hay muchos modelos, y cada uno tiene su propio tokenizador. Por suerte para nosotros, hay una clase `AutoTokenizer` que hace el trabajo pesado después de que proporcionamos el checkpoint."
      ],
      "metadata": {
        "id": "kQb9SHI-tNyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")  # loads dataset raw\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "gkzGKLjN-T9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notemos que es un diccionario con los datasets de entrenamiento, evaluacion y no supervisado."
      ],
      "metadata": {
        "id": "CnDCHDZ7trYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][0]  # Let's see the first review"
      ],
      "metadata": {
        "id": "1Q13wuHGtqu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero como sabemos si label 0 significa positivo o negativo? Tenemos que ver el atributo features"
      ],
      "metadata": {
        "id": "hz7V6Iw1uEJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = raw_datasets['train'].features['label']\n",
        "label.int2str(0)"
      ],
      "metadata": {
        "id": "HgQQCHo5t99k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahí está, dentro de características vemos que el índice 0 es **Negativo**\n",
        "\n",
        "Ahora, para tokenizar el conjunto de datos, necesitamos cargar el tokenizador adecuado para el modelo que nos interesa. ¡Y la vamos a aplicar en todas partes!\n",
        "\n",
        "Después de este paso, el tokenizador convierte el texto en un tensor de identificadores, cada uno de los cuales representa una palabra diferente en el vocabulario BERT."
      ],
      "metadata": {
        "id": "C7p9f6uYuDNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    # We are using the BERT tokenizer, specifying to PAD until the end,\n",
        "    # truncate if either 128 elements are met or the maximum from the model, which you get from the model card\n",
        "\n",
        "    return tokenizer(example[\"text\"], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "zZ0eyz_bswnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it worked!"
      ],
      "metadata": {
        "id": "oSt_kIFKvKwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets['train'][0]['text']"
      ],
      "metadata": {
        "id": "oM3_SxEHaYpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(tokenized_datasets['train'][0]['text'])"
      ],
      "metadata": {
        "id": "tu6DT9ARvqeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El tokenizador de BERT (bueno, DistillBERT) convierte cada palabra en su ID de acuerdo con *su* vocabulario. Y observe que el enmascaramiento dice que no hemos sido truncados. Lo que sí sabremos es hacer esto para todos los datos y convertirlos en un objeto TF Datasets (que Keras acepta)"
      ],
      "metadata": {
        "id": "db-I6E5EvymQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=['input_ids'],\n",
        "    label_cols=[\"label\"],\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
        "    columns=['input_ids'],\n",
        "    label_cols=[\"label\"],\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "_BkOcOV3YM-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, labels in tf_train_dataset.take(1):\n",
        "  print(f' inputs: {inputs.shape}, labels: {labels.shape}')\n"
      ],
      "metadata": {
        "id": "fm-TbT_LwMaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargar el modelo y prepararse para el entrenamiento"
      ],
      "metadata": {
        "id": "-AKKIC9AzDWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a descargar el modelo. Es muy importante que utilice la clase que comienza con `TFAutoModel`. Hay modelos automáticos para la mayoría de las tareas, por lo que no tiene que agregar manualmente el encabezado, por ejemplo, `TFAutoModelForSequenceClassification` agrega una capa Densa (SIN SOFTMAX) para hacer la clasificación"
      ],
      "metadata": {
        "id": "B5NK3y0Kvy9n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQ1RBW49vzi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a definir el optimizador y la funcion de perdida. Es muy important que la Perdida tenga `from_logits=True` porque los modelos de Hugging Face devuelven logits, no probabilidades."
      ],
      "metadata": {
        "id": "qg1xrLwimaug"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpdCdwVVc7av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos el modelo"
      ],
      "metadata": {
        "id": "cb4fZNJVn2hL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7wxa25Yc7l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y veamos el summary del modelo"
      ],
      "metadata": {
        "id": "geg3ux2Gn-V4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_4l0JM6e7tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Oh, no! ¡Tenemos demasiados parámetros para entrenar! Por suerte en Keras es muy fácil configurar algunas capas como no entrenables"
      ],
      "metadata": {
        "id": "w0IY68FyxQ5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2z7l4zzOglb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Voilá!*"
      ],
      "metadata": {
        "id": "2XwbQq6_xXjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5NEOuZkdJl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenemos el modelo!"
      ],
      "metadata": {
        "id": "iHA_MZQ01mgn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "coaX2kRYz6lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora tenemos un modelo entrenado que transfirió el aprendizaje de DistillBERT"
      ],
      "metadata": {
        "id": "bh7F4SHfiXjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probando el modelo"
      ],
      "metadata": {
        "id": "Ai85jC1HzGSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero probemos con dos oraciones, una negativa y una positiva. Vamos a tener que tokenizarlas que hicimos antes"
      ],
      "metadata": {
        "id": "Jbo7CY_0oYYV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G4Tw4BGWdPbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a analizar esos tokens"
      ],
      "metadata": {
        "id": "_hecP13Utw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "seclxXEgivkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que la primera oracion tuvo padding, como esperabamos. Vamos a predecir con estos input_ids"
      ],
      "metadata": {
        "id": "Arav2lidt0VV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFfLSI28i2IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notemos que el output de la predicciónson logits, no probabilidades. Por lo que conviene aqui aplicar `tf.math.softmax` al resultado."
      ],
      "metadata": {
        "id": "dptMVX4Yyz8B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h_aQ5cWxkBqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfecto, ahora si hacemos un `tf.math.argmax` vamos a obtener que clase es."
      ],
      "metadata": {
        "id": "y59IN1akuJT7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g07INFlujKAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para recordar las clases tenemos la variable `label` que preserva los indices del output de la softmax"
      ],
      "metadata": {
        "id": "mo3f2XZOuQsd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKoYbs9vrVYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo esta correcto!"
      ],
      "metadata": {
        "id": "xehvfI6Qy56b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si quisiesemos publicar el modelo primero evaluemoslo con el evaluation set"
      ],
      "metadata": {
        "id": "otDTc9Bnuk6d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTev4jIXjwRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psA6cEdy1adF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}